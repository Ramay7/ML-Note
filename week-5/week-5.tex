\documentclass[11pt]{article}
\usepackage[BoldFont,SlantFont,CJKchecksingle]{xeCJK}
\usepackage[top=0.5in,bottom=0.5in,left=1.25in,right=0.8in]{geometry}
\setCJKmainfont[BoldFont=SimHei]{SimSun}
\setCJKmonofont{SimSun}% 设置缺省中文字体
\parindent 0em   %段首缩进
%\usepackage{indentfirst}	%设置第一段也首行缩进
\linespread{1}	%设置行距
\addtolength{\parskip}{.4em}	%增加段间距0.4em
\usepackage{amsmath} % 插入数学公式
\usepackage[colorlinks,linkcolor=blue]{hyperref} 	%设置超链接

% 设置页眉页脚
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{} 
\chead{} 
\rhead{\bfseries {ramayzhu0625@gmail.com}} 
\lfoot{} 
\cfoot{}
\rfoot{\thepage} 
\renewcommand{\headrulewidth}{0.4pt} 
\renewcommand{\footrulewidth}{0.4pt}

% \includegraphics[width = .8\textwidth]{pic.png}  图片的宽度会被缩放至页面宽度的百分之八十，图片的总高度会按比例缩放 

\begin{document}
	\title{Machine Learning Week-5}
	\author{ramay7}
	
	\maketitle % 显示标题
	\tableofcontents % 生成目录
	%\newpage
	
	\section{What is BP(Back Propagation) Algorithm ?}
		Back Propagation is used to compute the partial derivation of $J(\theta)$ :$\frac{\partial}{\partial \theta_{i, j}^{l}} J(\theta)$.
			
		$$
		J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left[ \sum_{i=1}^{m}y^{(i)} \log(h_{\theta}(x^{(i)})) + (1-y^{(i)})\log (1-h_{\theta}(x^{(i)})) \right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}} (\theta_{j,i}^{l})^2
		$$
		
		The main reason that we use the back propagation algorithm rather than the numerical gradient computation method during learning is that the latter is very slow.
		
		The gradient for the sigmoid function can be computed as:
		
		$$
		g'(z) = \frac{d}{dz}g(z) = g(z)(1-g(z))
		$$
		where sigmoid(z) = g(z) = $\frac{1}{1+e^{-z}}$.
		
	\section{Random Initialization: Symmetry breaking}
		Initialize each $\theta_{i,j}^{(l)}$ to a random value in $\left[ -\varepsilon, \varepsilon \right]$ (i.e. $-\varepsilon \leq \theta_{i,j}^{(l)} \leq \varepsilon$).
		
		E.g. Theta1 = rand(10, 11) * (2 * INIT\_EPSILON) - INIT\_EPSILON. 		
		
		When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\theta^{(l)}$ uniformly in the range $\left[ -\varepsilon_{init}, \varepsilon_{init} \right]$. One effective strategy for choosing $\varepsilon_{init}$ is to base it on the number of units in the network. A good choice of $\varepsilon_{init}$ is $\varepsilon_{init} = \frac{\sqrt{6}}{\sqrt{L_{in} + L_{out}}}$, where $L_{in} = s_{l}$ and $L_{out} = s_{l+1}$ are the number of units in the layer adjacent to $\theta^{l}$.
		
	\section{Concrete Steps}
		\begin{itemize}
			\item {set $\Delta_{i, j}^{(l)} := 0$ for all (l, i, j)(hence you end up having a matrix full of zero)}
			\item{for i = 1 to m
				\begin{enumerate}
					\item Set $a^{(1)} = x^{(i)}$
					\item Perfom forward propagation to compute $a^{(l)}$ for $l=2,3,...,L$
					$$
					a^{(l)} = g(z^{(l)}) = g(\theta ^{(l-1)}a^{(l - 1)}))
					$$
					\item Using $y^{(i)}$, compute $\delta ^{(L)} = a^{(L)} - y^{(i)}$
					\item Computing $\delta^{(L-1)}, \delta^{(L-2)}, ..., \delta^{(2)}$ using $\delta^{(l)} = (\theta^{(l)})^{T}\delta^{l+1} .* a^{(l)} .* (1-a^{(l)})$
					\item $\Delta_{i,j}^{(l)} := \Delta_{i,j}^{(l)} + a_{j}^{(l)} \delta_{j}^{(l+1)}$ or with vectorization, $\Delta^{l} := \Delta^{l} + \delta^{l+1}(a^{l})^T$
				\end{enumerate}
				}
			\item Hence we update our new $\Delta$ matrix:
			
			\begin{equation*}
				D_{i,j}^{(i)} := 
				\begin{cases}
					\frac{1}{m} \Delta_{i,j}^{(l)}  &\mbox{j = 0} \\
					\frac{1}{m}(\Delta_{i,j}^{(l)} + \lambda \theta_{i,j}^{(l)})  &\mbox{j $\neq$ 0}
				\end{cases}
			\end{equation*}
		\end{itemize}
		
		The captial-delta matrix D is used as an "accumulator" to add up our values as we go along and eventually compute our partial derivative. Thus, we get:
		
		$$
		\frac{\partial}{\partial \theta_{i, j}^{(l)}}J(\theta) = D_{i, j}^{(l)}
		$$
	
	
	\section{The End} 
		To be honest, BP algorithm is a little bit hard to understand, and a little difference may cause a compelety wrong answer while doing exercise. In addition, I have written \href{https://ramay7.github.io/2017/03/01/Some-proof-about-BP-Algorithm/}{some proof about BP Algorithm}. This may be helpful to understand the algorithm.

\end{document}