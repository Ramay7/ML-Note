\documentclass[11pt]{article}
\usepackage[BoldFont,SlantFont,CJKchecksingle]{xeCJK}
\usepackage[top=0.5in,bottom=0.5in,left=1.25in,right=0.8in]{geometry}
\setCJKmainfont[BoldFont=SimHei]{SimSun}
\setCJKmonofont{SimSun}% 设置缺省中文字体
\parindent 0em   %段首缩进
%\usepackage{indentfirst}	%设置第一段也首行缩进
\linespread{1}	%设置行距
\addtolength{\parskip}{.4em}	%增加段间距0.4em
\usepackage{amsmath} % 插入数学公式
\usepackage[colorlinks,linkcolor=blue]{hyperref} 	%设置超链接

% 设置页眉页脚
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{} 
\chead{} 
\rhead{\bfseries {ramayzhu0625@gmail.com}} 
\lfoot{} 
\cfoot{}
\rfoot{\thepage} 
\renewcommand{\headrulewidth}{0.4pt} 
\renewcommand{\footrulewidth}{0.4pt}

% \includegraphics[width = .8\textwidth]{pic.png}  图片的宽度会被缩放至页面宽度的百分之八十，图片的总高度会按比例缩放 

\begin{document}
	\title{Machine Learning Week-3}
	\author{ramay7}
	
	\maketitle % 显示标题
	\tableofcontents % 生成目录
	%\newpage
	
	\section{Classification and Representation}
		
		\subsection{Hypothesis Representation}
		Want $0\leq h_{\theta}(x) \leq 1$:
		
		$$
		h_{\theta}(x) = g(X\theta)
		$$
		where $g(z) = \frac{1}{1+e^{-z}}$ is called \textbf{sigmoid/logistic function}. $h_{\theta}(x)$ will give us the \textbf{probability that our output is 1}.
		
		\subsection{Decision Boundary}
		For:
		$$g(X\theta)=\frac{1}{1+e^{-X\theta}}$$
		assume $X\theta = 0$, and then get an equation.
	
	\section{Logistic Regression Model}
		
		\subsection{Cost Function}
			$$
			J(\theta) = \frac{1}{m}\sum_{i=1}^{m}CostFunction(h_{\theta}(x^{(i)}) ,y^{(i)})
			$$
			
			For linear regression:
			
			$$Cost(h_{\theta}(x) ,y) = \frac{1}{2}(h_{\theta}(x) - y)^2$$
			
			For logistic regression:
			
			\begin{equation}
				Cost(h_{\theta}(x) ,y)=
				\begin{cases}
			
					-\log(h_{\theta}(x)) &\mbox{if y = 1} \\
			
					-\log(1- h_{\theta}(x)) &\mbox{if y = 0}
			
				\end{cases}
			\end{equation}
			
			This form of $Cost(h_{\theta}(x) ,y)$ guarantees that $J(\theta)$ is \textbf{convex} for logistic regression.
			
			We can change the cost function of logistic regression into another form:
			
			$$
			Cost(h_{\theta}(x) ,y)= -y\log(h_{\theta}(x)) - (1-y)\log(1- h_{\theta}(x))
			$$
			
			Hence, we can get:
			
			$$
			J(\theta) = \frac{1}{m}(-y^{T}\log(h) - (1-y)^{T}\log (1-h))
			$$
			
			To make it concrete:
			
			\begin{center}
				\begin{align}
					J(\theta) &=- \frac{1}{m} \sum_{i=1}^{m} y^{(i)}\log(h_{\theta}(x^{(i)})) + (1-y^{i})\log (1-h_{\theta}(x^{i})) \\
					\frac{\partial J(\theta)}{\partial \theta _j} &= \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)}
				\end{align}
			\end{center}
			
		\subsection{A Vectorized Implementation for Gradient Descent}
		$$
		\theta := \theta - \frac{\alpha}{m} X^{T}(g(X\theta) - y)
		$$
	
	\section{Multiclass Classfication: One-vs-all}
	
	Train a logistic regression classifier $h_{\theta}^{(i)}$ for each class $i$ to predict the probability that $y = 1$.
	
	On a new input $x$ to make a prediction, pick the class $i$ that maximizes $\max_{i} h_{\theta}^{(i)}(x)$
	
	\section{Solving the Problem of Overfitting}
	
		\subsection{The Problem of Overfitting}
			Underfitting is when the form of our hypothesis function $h$ maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. As the other extremes, overfitting is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.
			
			This terminology is applied to both linear and logistic regression. There are two main options to adress the issue of overfitting:
			\begin{itemize}
				\item Reduce the number of features:
				\begin{itemize}
					\item Manually select which features to keep
					\item Use a model selection algorithm
				\end{itemize}
				\item Regularization
				\begin{itemize}
					\item Keep all the features, but reduce the magnitude of parameters $\theta_j$
					\item Regularization works well when we have a lot of slightly useful features
				\end{itemize}
			\end{itemize}
		
		\subsection{Cost Function in Regularization}
		
		$$
		J(\theta) = \frac{1}{2m} \left[  \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{m}\theta_{j}^2 \right]
		$$
		where $\lambda$ is called \textbf{regularization parameter}.
		
		\textbf{Please take care that the index of the quadratic sum of $\theta$ starts from 1.} That is to say, ignore $\theta_{0}^2$.
		
		If $\lambda$ is set to an extremely large value (perhaps for too large for our problem, say $\lambda = 10^{10}$), it may smooth out the function too much and cause underfitting.
		
		\subsection{Regularized Linear Regression}
		
		The Gradient Descent is:
			\textbf{repeat until convergence:\{ 
				\begin{center}
					\begin{align}
						\theta_{0} &:= \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m}( h_{\theta}(x^{(i)}) - y^{(i)} ) x_{0}^{(i)}\\
						\theta_{j} &:= \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} + \lambda \theta_{j} \qquad for\ j \geq 1
					\end{align}
				\end{center}
				\qquad \} 
			}
			
		\subsection{Normal Equation}
		$$\theta = ( X^{T}X + \lambda L )^{-1} X^{T}y$$
		where L is a $(n+1)$ by $(n+1)$ matirx:
		$$L=\left[
			\begin{array}{ccccc}
				0 & 0 & 0 & \cdots & 0 \\
				0 & 1 & 0 & \cdots & 0 \\
				0 & 0 & 1 & \cdots & 0 \\
				. & . & . & . & . \\
				. & . & . & . & . \\
				. & . & . & . & . \\
				0 & 0 & 0 & \cdots & 1
			\end{array}
		\right]$$
		
		If $m \leq n$, then $X^{T}X$ is non-invertible. However, when we add the term L, then $X^{T}X + \lambda L$ becomes invertiable.
		
		\subsection{Regularized Logistic Regression}
		
		Cost Function:
		$$
		J(\theta) =- \left[ \frac{1}{m} \sum_{i=1}^{m} y^{(i)}\log(h_{\theta}(x^{(i)})) + (1-y^{i})\log (1-h_{\theta}(x^{i}))\right] + \frac{\lambda}{2m}\sum_{j=1}^{m}\theta_{j}^2
		$$
		
		\textbf{Please take care that the index of the quadratic sum of $\theta$ starts from 1.} That is to say, ignore $\theta_{0}^2$.
		
		Gradient descent is the same with regularized linear regression except that $h_{\theta}(X) = \frac{1}{1+e^{-\theta^{T} X}}$.
		
		\begin{center}
			\begin{align}
			\frac{\partial J(\theta)}{\partial\theta_0} &:= \theta_{0} - \frac{1}{m} \sum_{i=1}^{m}( h_{\theta}(x^{(i)}) - y^{(i)} ) x_{0}^{(i)}\\
			\frac{\partial J(\theta)}{\partial\theta_j} &:= \left( \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} \right) + \frac{\lambda}{m} \theta_{j} \qquad for\ j \geq 1
			\end{align}
		\end{center}
		
	\section{The End} 
		The most wonderful job is getting full marks for Exercise-2.

\end{document}