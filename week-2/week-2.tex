\documentclass[11pt]{article}
\usepackage[BoldFont,SlantFont,CJKchecksingle]{xeCJK}
\usepackage[top=0.5in,bottom=0.5in,left=1.25in,right=0.8in]{geometry}
\setCJKmainfont[BoldFont=SimHei]{SimSun}
\setCJKmonofont{SimSun}% 设置缺省中文字体
\parindent 2em   %段首缩进
\usepackage{indentfirst}	%设置第一段也首行缩进
\linespread{1}	%设置行距
\addtolength{\parskip}{.4em}	%增加段间距0.4em
\usepackage{amsmath} % 插入数学公式
\usepackage[colorlinks,linkcolor=blue]{hyperref} 	%设置超链接

% 设置页眉页脚
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{} 
\chead{} 
\rhead{\bfseries {ramayzhu0625@gmail.com}} 
\lfoot{} 
\cfoot{}
\rfoot{\thepage} 
\renewcommand{\headrulewidth}{0.4pt} 
\renewcommand{\footrulewidth}{0.4pt}

% \includegraphics[width = .8\textwidth]{pic.png}  图片的宽度会被缩放至页面宽度的百分之八十，图片的总高度会按比例缩放 

\begin{document}
	\title{Machine Learning Week-2}
	\author{ramay7}
	
	\maketitle % 显示标题
	\tableofcontents % 生成目录
	%\newpage
	
	\section{Multivariate Linera Regression}
		
		The multivariable form of the hypothesis function accommodating these multiple features is as follows:
		
		$$
		h_{\theta}(x) = \theta _0 + \theta _{1}x_{1} + \theta _{2}x_{2} + \cdots + \theta _{n}x_{n}
		$$
		
		In addition, if we set $x_{0}^{(i)} = 1$, we can get:
		
		$$
		h_{\theta}(X) = X\theta $$
		
		$$ J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2 
		$$
		
		The Gradient Descent is: 
			\textbf{repeat until convergence:\{ 
				\begin{center}
					\begin{align}
						\theta_{j} &:= \theta_{j} - \alpha \frac{1}{m} \frac{\partial}{\partial\theta_{j}} J(\theta) \\
						&:= \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}
					\end{align}
					for j = 0, 1, 2,...,n
				\end{center}
				\qquad \} 
			}
			
		We can speed up gradient descent by having each of our input values in roughly the same range, such as:
		
		
		$$-1\leq x_{(i)}\leq 1 \ or\ -0.5\leq x\leq 0.5$$
			
		Two techniques to help with this are \textbf{feature scaling} and \textbf{mean normalization}.
		
		$$x_i := \frac{x_i - \mu_i}{s_i}$$
		
		Where $\mu_i$ is the \textbf{average} of all the value for feature(i) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation.
		
		\textbf{By the way, in the \underline{ex1.pdf}, there is a introduction to 'standard deviation'.} The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within $\pm 2$ standard deviations of the mean); this is an alternative to taking the range of values (max - min). In MATLAB, we can use the "std" function to compute the standard devision.
		
		We can \textbf{combine} multiple features into one. For example, we can combine $x_1$ and $x_2$ into a new feature $x_3$ by taking $x_1\dot x_2$. We can \textbf{change the behavior or curve} of our hypothesis function by making it a quadratic, cubic, or square root function (or any other form).
		

	\section{Computing Parameters Analytically(Normal Equation)}
	
		It is no doubt that gradient descent gives one way of minizing J. Let's discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. The normal equation formula is given below:
		
		$$
		\theta = (X^{T}X)^{-1}X^{T}y
		$$
		
		\textbf{There is no need to do feature scaling with the normal equation.}
		
		The following is a comparison of gradient descent and the normal equation:
		
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Gradient Descent} & \textbf{Normal Equation} \\
			\hline
			Need to choose alpha & No need to choose aplha \\
			\hline
			Needs many iterations & No need to iterate \\
			\hline
			O($kn^2$) & O($n^3$), need to calculate inverse of $X^{T}X$ \\
			\hline 
			Works well when n is large & Slow if n is very large \\
			\hline
		\end{tabular}
		
		When implementing the normal equation in MATLAB, we want to use the 'pinv' function rather than 'inv'. The 'pinv' function will give you a value of $\theta$ even if $X^{T}X$ is not invertible.
		
		if $X^{T}X$ is \textbf{noninvertible}, the common causes might be having:
		
		\begin{itemize}
		\item Redundant features, where two features are very closely related (i.e. they are lineraly dependent)
		\item Too many features (e.g. m $\leq$ n). In this case, delete some features or use "regularization" .
		\end{itemize}
		
		Solutions to the above problems include deleting a feature that is lineraly dependent with another or deleting one or more features when there are too many features.

	\section{The End} 
		The most important harvest for me is finishing Exercise 1 completely, including the optional exercises. By this, I have got a little familiar with MATLAB. At last, I have got my first machine learning model for predicting the prices of houses roughly. You can get the source code in my \underline{week-2 file}.


\end{document}